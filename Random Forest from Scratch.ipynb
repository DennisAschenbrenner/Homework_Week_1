{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOM FOREST FROM SCRATCH ##\n",
    "\n",
    "\n",
    "class RandomForest():      # define class of RANDOMFOREST\n",
    "    def __init__(self, x, y, n_trees, n_features, sample_sz, depth=10, min_leaf=5): # init \n",
    "        np.random.seed(12)     # setting a random seed\n",
    "        if n_features == 'sqrt':   # condition: if given n_features are refered as squareroot then\n",
    "            self.n_features = int(np.sqrt(x.shape[1])) # calculate squareroot of col number of x\n",
    "        elif n_features == 'log2': # condition 2: if refered to log2 then\n",
    "            self.n_features = int(np.log2(x.shape[1]))  # calculate log2 of column number of x\n",
    "        else:\n",
    "            self.n_features = n_features   # if no condition given then do nothing\n",
    "        print(self.n_features, \"sha: \",x.shape[1])  # print number of columns of x   \n",
    "        # write in refered data to self-variables\n",
    "        self.x, self.y, self.sample_sz, self.depth, self.min_leaf  = x, y, sample_sz, depth, min_leaf\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)] # refer create tree in range of n_trees\n",
    "        \n",
    "        # sample_size: the number of rows randomly selected and passed onto each tree. This is usually equal \n",
    "        # to total number of rows but can be reduced to increase performance and decrease correlation of trees \n",
    "        # in some cases (bagging of trees is a completely separate machine learning technique)\n",
    "    \n",
    "    def create_tree(self):  # function to create a decision tree\n",
    "        # create random index permutation of sample size, depending on y-length  \n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz] \n",
    "        # random permutation of sample size in the length of y\n",
    "        # indxs: this parameter exists to keep track of which indices of the original set goes to the right \n",
    "        # and which goes to the left tree. Hence every tree has this parameter “indxs” which stores the indices \n",
    "        # of the rows it contains. Prediction is made by averaging these rows.\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features] # permutation of columns (features)\n",
    "        # for the x column length\n",
    "        # return decisiontree of permuted indexes of x and y with given n-features, permuted columns, \n",
    "        # and an np.array in the range of sample size, given depth of DT and min number of leafs\n",
    "        return DecisionTree(self.x.iloc[idxs], self.y[idxs], self.n_features, f_idxs,\n",
    "                    idxs=np.array(range(self.sample_sz)),depth = self.depth, min_leaf=self.min_leaf)\n",
    "        \n",
    "        # min_leaf: minimum number of rows required in a node to cause further split. Lower the min_leaf, \n",
    "        # higher the depth of the tree.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self, x):    # function to predict x_pred of x (), avaraging of predictions\n",
    "        return np.mean([t.predict(x) for t in self.trees], axis=0)\n",
    "\n",
    "def std_agg(cnt, s1, s2): return math.sqrt((s2/cnt) - (s1/cnt)**2) # function to calculate standard deviation\n",
    "                                                                   # \n",
    "class DecisionTree():    # class for decision tree\n",
    "    \n",
    "    def __init__(self, x, y, n_features, f_idxs,idxs,depth=10, min_leaf=5): # init with defaults\n",
    "        # write in given data and properties\n",
    "        self.x, self.y, self.idxs, self.min_leaf, self.f_idxs = x, y, idxs, min_leaf, f_idxs\n",
    "        self.depth = depth\n",
    "        print(f_idxs)\n",
    "#         print(self.depth)\n",
    "        self.n_features = n_features\n",
    "        self.n, self.c = len(idxs), x.shape[1]   # n = length of permuted indices and c is number of cols of x\n",
    "        self.val = np.mean(y[idxs])              # val is mean of y with permuted indices\n",
    "        self.score = float('inf')                # score = float for infinit \n",
    "        self.find_varsplit()                     # refering the next function\n",
    "        \n",
    "    def find_varsplit(self):                     # function varsplit\n",
    "        for i in self.f_idxs: self.find_better_split(i) # looping through find_better_split\n",
    "        if self.is_leaf: return                         \n",
    "        x = self.split_col                       # write vals of split_col to x\n",
    "        lhs = np.nonzero(x<=self.split)[0]       # write indices for non-zero values of x <= split to lhs\n",
    "        rhs = np.nonzero(x>self.split)[0]        # write indices for non-zero values of x > split to rhs\n",
    "        # rhs stores the indices of rows passed onto the right decision tree.\n",
    "        # lhs to the left decision tree\n",
    "        \n",
    "        # random permutations in range of cols of x  \n",
    "        lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        self.lhs = DecisionTree(self.x, self.y, self.n_features, lf_idxs, self.idxs[lhs], \n",
    "                                depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        self.rhs = DecisionTree(self.x, self.y, self.n_features, rf_idxs, self.idxs[rhs], \n",
    "                                depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "\n",
    "    def find_better_split(self, var_idx):    # function to find a better data split\n",
    "        x, y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
    "        sort_idx = np.argsort(x)             # save sorted indices of x\n",
    "        sort_y,sort_x = y[sort_idx], x[sort_idx] # choose y's for sorted indices of x\n",
    "        # set rhs_cnt to length of random permuted indices n, rhs_sum to sum of sorted y's and sum squared\n",
    "        rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "        # set lhs count and lhs sums to zeros\n",
    "        lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "\n",
    "        for i in range(0,self.n-self.min_leaf-1): # for loop in range of zero to n-permuted indices minus \n",
    "                                                  # minimum number of leafes minus 1\n",
    "            xi,yi = sort_x[i],sort_y[i]         \n",
    "            lhs_cnt += 1; rhs_cnt -= 1            # sum up lhs_cnt by 1 and rhs_cnt held at zero by -1\n",
    "            lhs_sum += yi; rhs_sum -= yi          # same for sum\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2  # same for sum squared\n",
    "            if i<self.min_leaf or xi==sort_x[i+1]:# condition continue if i<minimum leaf number or if \n",
    "                                                  # xi equal sorted x at i+1\n",
    "                continue\n",
    "\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2) # calc standard deviations for left decision tree\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2) # same for right decision tree\n",
    "            # current score = the split score for each iteration is simply the weighted average of standard \n",
    "            # deviation of the two halves with number of rows in each half as their weights. Lower score \n",
    "            # facilitates lower variance, lower variance facilitates grouping of similar data which will \n",
    "            # lead to better predictions.\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            if curr_score<self.score: \n",
    "                self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
    "            # whenever the current score is better (less than the currently saved score in self.score) we will \n",
    "            # update the current score and store this column in the variable self.var_idx (remember this is the \n",
    "            # variable that helps selecting the column for two of our property decorators) and save the value \n",
    "            # upon which the split is made, in the variable self.split\n",
    "\n",
    "    @property\n",
    "    def split_name(self): return self.x.columns[self.var_idx]\n",
    "    #A property decorator to return the name of the column we’re splitting over. var_idx is the index of this \n",
    "    # column, we will calculate this index in the find_better_split function along with value of the column we \n",
    "    # split upon\n",
    "    \n",
    "    @property\n",
    "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "    # A property decorator to return the column at index var_idx with elements at indices given by indxs variable. \n",
    "    # Basically, segregates a column with selected rows.\n",
    "    \n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf') or self.depth <= 0 \n",
    "    # A leaf node is the one that has never made a split thus it has a score of infinite (as mentioned above) \n",
    "    # hence this functions is used to identify leaf nodes. Also in case we’ve crossed maximum depth i.e. \n",
    "    # self.depth <= 0, it’s a leaf node as we can’t go any deeper.\n",
    "\n",
    "    def predict(self, x):   \n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "        return t.predict_row(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,3,9])\n",
    "np.argsort(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
